#!/usr/bin/osascript -l JavaScript

// Helpers
function envVar(varName) {
  return $.NSProcessInfo
    .processInfo
    .environment
    .objectForKey(varName).js
}

function fileExists(path) {
  return $.NSFileManager.defaultManager.fileExistsAtPath(path)
}

function fileModified(path) {
  return $.NSFileManager.defaultManager
    .attributesOfItemAtPathError(path, undefined)
    .js["NSFileModificationDate"].js
    .getTime()
}

function deleteFile(path) {
  return $.NSFileManager.defaultManager.removeItemAtPathError(path, undefined)
}

function writeFile(path, text) {
  $(text).writeToFileAtomicallyEncodingError(path, true, $.NSUTF8StringEncoding, undefined)
}

function readChat(path) {
  const chatString = $.NSString.stringWithContentsOfFileEncodingError(path, $.NSUTF8StringEncoding, undefined).js
  return JSON.parse(chatString)
}

function appendChat(path, message) {
  const ongoingChat = readChat(path).concat(message)
  const chatString = JSON.stringify(ongoingChat)
  writeFile(path, chatString)
}

function markdownMessage(message){
  const reasoningContent = (message?.reasoningContent || "")
        .split("\n")
        .filter(item => item)
        .map(item => `>${item}`).join("\n>\n") // format reasoning text with quote sign >
  return reasoningContent ? `${reasoningContent}\n\n${message["content"]}` : message["content"]
}

function markdownChat(messages, ignoreLastInterrupted = true) {
  return messages.reduce((accumulator, current, index, allMessages) => {
    if (current["role"] === "assistant")
        return `${accumulator}${markdownMessage(current)}\n\n`
    if (current["role"] === "user") {
      const userMessage = `# ⊙ You\n\n${current["content"]}\n\n# ⊚ Assistant`
      const userTwice = allMessages[index + 1]?.["role"] === "user" // "user" role twice in a row
      const lastMessage = index === allMessages.length - 1 // "user is last message

      return userTwice || (lastMessage && !ignoreLastInterrupted) ?
        `${accumulator}${userMessage}\n\n[Answer Interrupted]\n\n` :
        `${accumulator}${userMessage}\n\n`
    }

    // Ignore any other role
    return accumulator
  }, "")
}


/**
 * 合并连续相同角色的消息
 * @param {Array} messages 原始消息数组
 * @returns {Array} 合并后的消息数组
 */
function mergeConsecutiveMessages(messages) {
  return messages.reduce((merged, current) => {
    const lastMsg = merged[merged.length - 1];
    if (lastMsg && lastMsg.role === current.role) {
      // 合并连续相同角色的消息内容
      lastMsg.content += `\n${current.content}`;
    } else {
      merged.push({ ...current });
    }
    return merged;
  }, []);
}

function startStream(apiEndpoint, apiKey, apiOrgHeader, model, systemPrompt, contextChat, streamFile, pidStreamFile, timeoutSeconds) {
  $.NSFileManager.defaultManager.createFileAtPathContentsAttributes(streamFile, undefined, undefined) // Create empty file

  const messages = systemPrompt ?
    [{ role: "system", content: systemPrompt }].concat(contextChat) :
    contextChat

  const task = $.NSTask.alloc.init
  const stdout = $.NSPipe.pipe

  task.executableURL = $.NSURL.fileURLWithPath("/usr/bin/curl")
  task.arguments = [
    apiEndpoint,
    "--speed-limit", "0", "--speed-time", timeoutSeconds.toString(), // Abort stalled connection after a few seconds
    "--silent", "--no-buffer",
    "--header", "Content-Type: application/json",
    "--header", `Authorization: Bearer ${apiKey}`,
    "--data", JSON.stringify({ model: model, messages: messages, stream: true }),
    "--output", streamFile
  ].concat(apiOrgHeader)

  task.standardOutput = stdout
  task.launchAndReturnError(false)
  writeFile(pidStreamFile, task.processIdentifier.toString())
}

function readStream(streamFile, chatFile, pidStreamFile, timeoutSeconds) {
  const streamMarker = envVar("stream_marker") === "1"
  const streamString = $.NSString.stringWithContentsOfFileEncodingError(streamFile, $.NSUTF8StringEncoding, undefined).js

  // When starting a stream or continuing from a closed window, add a marker to determine the location of future replacements
  if (streamMarker) return JSON.stringify({
    rerun: 0.1,
    variables: { streaming_now: true },
    response: "…",
    behaviour: { response: "append" }
  })

  // If response looks like proper JSON, it is probably an error
  if (streamString.startsWith("{")) {
    try {
      streamJSON = JSON.parse(streamString)
      const errorMessage = streamJSON["status"] && streamJSON["message"] ? streamJSON["message"] : JSON.parse(streamString)["error"]["message"]
      
      if (errorMessage) {
        // Delete stream files
        deleteFile(streamFile)
        deleteFile(pidStreamFile)

        return JSON.stringify({
          response: `[${errorMessage}]`, // Surround in square brackets to look like other errors
          behaviour: { response: "replacelast" }
        })
      }

      throw "Could not determine error message" // Fallback to the catch
    } catch {
      // If it's not an error from the API, log file contents
      console.log(streamString)

      return JSON.stringify({
        response: streamString,
        behaviour: { response: "replacelast" }
      })
    }
  }

  // Parse streaming response
  const chunks = streamString
    .split("\n") // Split into lines
    .filter(item => item) // Remove empty lines
    .map(item => item.replace(/^data: /, "")) // Remove extraneous "data: "
    .flatMap(item => { try { return JSON.parse(item) } catch { return [] } }) // Parse as JSON

  const responseText = chunks.map(item => item["choices"][0]?.["delta"]["content"]).join("")
  const responseReasoningText = chunks.map(item => item["choices"][0]?.["delta"]["reasoning_content"]).join("")
  
  const finalRespText = markdownMessage({content: responseText, reasoningContent: responseReasoningText})
  
    
  // If File not modified for over a few seconds, connection stalled
  const stalled = new Date().getTime() - fileModified(streamFile) > timeoutSeconds * 1000

  if (stalled) {
    // Write incomplete response
    if (responseText.length > 0) appendChat(chatFile, { role: "assistant", content: responseText, reasoningContent: responseReasoningText })

    // Delete stream files
    deleteFile(streamFile)
    deleteFile(pidStreamFile)

    // Stop
    return JSON.stringify({
      response: `${finalRespText} [Connection Stalled]`,
      footer: "You can ask LLM to continue the answer",
      behaviour: { response: "replacelast", scroll: "end" }
    })
  }

  // If file is empty, we were too fast and will try again on next loop
  if (streamString.length === 0) return JSON.stringify({
    rerun: 0.1,
    variables: { streaming_now: true }
  })

  // Last token finish reason
  const finishReason = chunks.slice(-1)[0]["choices"][0]["finish_reason"]
  // If reponse is not finished, continue loop
  if (!finishReason) return JSON.stringify({
    rerun: 0.1,
    variables: { streaming_now: true },
    response:  finalRespText,
    behaviour: { response: "replacelast", scroll: "end" }
  })

  // When finished, write history and delete stream files
  appendChat(chatFile, { role: "assistant", content: responseText, reasoningContent: responseReasoningText})
  // appendChat(chatFile, { role: "system", content: responseReasoningText })
  deleteFile(streamFile)
  deleteFile(pidStreamFile)

  // Mention finish reason in footer
  const footerText = (function() {
    switch (finishReason) {
      case "legth": return "Maximum number of tokens reached"
      case "content_filter": return "Content was omitted due to a flag from OpenAI content filters"
    }
  })()

  // Stop
  return JSON.stringify({
    response: finalRespText,
    footer: footerText,
    behaviour: { response: "replacelast", scroll: "end" }
  })
}

// Main
function run(argv) {
  // Constant data
  const typedQuery = argv[0]
  const maxContext = parseInt(envVar("max_context"))
  const timeoutSeconds = parseInt(envVar("timeout_seconds"))
  const apiKey = envVar("friday_app_id")
  const apiOrgHeader = envVar("openai_org_id") ? ["--header", `OpenAI-Organization: ${envVar("openai_org_id")}`] : []
  const apiEndpointInit = envVar("api_endpoint") || "https://aigc.sankuai.com/v1/openai/native/chat/completions"
  const apiEndpointInitHasPath = apiEndpointInit.replace(/^https?:\/\//, "").includes("/")
  const apiEndpoint = apiEndpointInitHasPath ? apiEndpointInit : `${apiEndpointInit}/v1/chat/completions`
  const systemPrompt = envVar("system_prompt")
  const model = envVar("model_override") ? envVar("model_override") : envVar("friday_model")
  const chatFile = `${envVar("alfred_workflow_data")}/chat.json`
  const pidStreamFile = `${envVar("alfred_workflow_cache")}/pid.txt`
  const streamFile = `${envVar("alfred_workflow_cache")}/stream.txt`
  const streamingNow = envVar("streaming_now") === "1"

  // If continually reading from a stream, continue that loop
  if (streamingNow) return readStream(streamFile, chatFile, pidStreamFile, timeoutSeconds)

  // Load previous conversation
  const previousChat = readChat(chatFile)

  // If "streaming_now" is unset but stream file exists, the window was closed mid-stream
  // Reload conversation and rerun to resume stream
  if (fileExists(streamFile)) return JSON.stringify({
    rerun: 0.1,
    variables: { streaming_now: true, stream_marker: true },
    response: markdownChat(previousChat, true),
    behaviour: { scroll: "end" }
  })

  // If argument is empty, return previous conversation
  if (typedQuery.length === 0) return JSON.stringify({
    response: markdownChat(previousChat, false),
    behaviour: { scroll: "end" }
  })

  // Append new question to chat
  const appendQuery = { role: "user", content: typedQuery }
  const ongoingChat = previousChat.concat(appendQuery)
  // filter reasoning content
  const contextChat = ongoingChat.slice(-maxContext).map(({role, content}) => ({role, content}))
  // 判断model是否以'deepseek-reasoner'开头
  const isDeepseekReasoner = model.includes("deepseek-r1") || model.includes("deepseek-reasoner")
  // Make API request, write chat file, and start loop
  startStream(apiEndpoint, apiKey, apiOrgHeader, model, systemPrompt, isDeepseekReasoner ? mergeConsecutiveMessages(contextChat):contextChat, streamFile, pidStreamFile, timeoutSeconds)
  appendChat(chatFile, appendQuery)

  return JSON.stringify({
    rerun: 0.1,
    variables: { streaming_now: true, stream_marker: true },
    response: markdownChat(ongoingChat)
  })
}
